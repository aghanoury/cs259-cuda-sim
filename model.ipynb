{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tital V100 Model\n",
    "We'll define some system parameters and then define the model.\n",
    "\n",
    "This model, for simplicitys sake, could be broken down into three main inputs\n",
    "\n",
    "1. GPU parameters\n",
    "2. Convolutional Parameters\n",
    "3. \"CUDA\" parameters (i.e. how the Conv calculations are divided amongst cuda cores)\n",
    "\n",
    "Given the first 2 inputs, we should be able to define a roofline model. Given the 3rd input, we can then find the point at which our \"cuda\" implementation lies on said model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll define the parameters for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolution model parameters\n",
    "\n",
    "# first layer is input, RGB image so that gives us three layers\n",
    "input_layers = {\n",
    "    'dimx': 256,\n",
    "    'dimy': 256,\n",
    "    'depth': 3,\n",
    "}\n",
    "\n",
    "kernel_parameters = {\n",
    "    \"x\": 5,\n",
    "    \"y\": 5,\n",
    "    \"depth\": input_layers[\"depth\"],\n",
    "    \"padding\": 3,\n",
    "    \"stridex\": 1,\n",
    "    \"stridey\": 1,\n",
    "}\n",
    "output_layers = {\n",
    "    'dimx': 256,\n",
    "    'dimy': 256,\n",
    "    'depth': 64,\n",
    "}\n",
    "\n",
    "system_latency_parameters = {\n",
    "    \"multiply\": 1,  # Ops per cycle\n",
    "    \"scratchpad_mem_access\": 1,  # Ops per cycle\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll make some definitivitve calculations about our model. Total number of bytes and total operations. These should be constant regardless of how we allocate our problem space in CUDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total bytes: 16.768310546875 MB\n",
      "Total ops: 0.3145728 GFLOPS\n"
     ]
    }
   ],
   "source": [
    "# the definitative total number of operations for this convolution.\n",
    "total_bytes = output_layers['dimx'] * output_layers['dimy'] * output_layers['depth'] * 4 # 4 bytes per float\n",
    "\n",
    "# every \n",
    "total_bytes += kernel_parameters['x'] * kernel_parameters['y'] * kernel_parameters['depth'] * output_layers['depth'] * 4 # 4 bytes per float\n",
    "total_bytes += input_layers['dimx'] * input_layers['dimy'] * input_layers['depth'] * 4 # 4 bytes per float\n",
    "\n",
    "# total_ops = input_layers['dimx'] * input_layers['dimy'] * input_layers['depth'] * kernel_parameters['x'] * kernel_parameters['y'] * kernel_parameters['depth']\n",
    "total_ops = output_layers['dimx'] * output_layers['dimy'] * output_layers['depth'] * kernel_parameters['x'] * kernel_parameters['y'] * kernel_parameters['depth']\n",
    "\n",
    "# print the total bytes in MB\n",
    "print(\"Total bytes: {} MB\".format(total_bytes / 1024 / 1024))\n",
    "\n",
    "# print the total ops in GFLOPS\n",
    "print(\"Total ops: {} GFLOP\".format(total_ops / 1000 / 1000 / 1000))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
